<div align="center">
  <img src="figures/AgentMemoryBench.svg" width="100%" alt="Agent Memory Bench" />

  <br/>
  <br/>

  <a href="https://github.com/s010m00n/AgentMemoryBench/stargazers">
    <img src="https://img.shields.io/github/stars/s010m00n/AgentMemoryBench?style=for-the-badge&logo=github&color=ff6b6b" alt="Stars">
  </a>
  <a href="https://github.com/s010m00n/AgentMemoryBench/network/members">
    <img src="https://img.shields.io/github/forks/s010m00n/AgentMemoryBench?style=for-the-badge&logo=github&color=ee5a6f" alt="Forks">
  </a>
  <a href="https://github.com/s010m00n/AgentMemoryBench/issues">
    <img src="https://img.shields.io/github/issues/s010m00n/AgentMemoryBench?style=for-the-badge&logo=github&color=c44569" alt="Issues">
  </a>
  <a href="https://github.com/s010m00n/AgentMemoryBench/blob/main/LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-brightgreen?style=for-the-badge" alt="License">
  </a>

  <br/>
  <br/>

  <p align="center">
    <strong>A Unified Benchmark for Continual Agent Memory</strong>
    <br />
    <br />
    A comprehensive benchmark for evaluating memory mechanisms in LLM-based agents across continual learning scenarios, supporting both <strong>system memory</strong> (task workflows) and <strong>personal memory</strong> (user preferences).
    <br />
    <br />
    <a href="#overview">Overview</a> ‚Ä¢
    <a href="#evaluation-modes">Evaluation Modes</a> ‚Ä¢
    <a href="#quick-start">Quick Start</a> ‚Ä¢
    <a href="#creating-custom-memory-mechanisms">Custom Memory</a> ‚Ä¢
    <a href="#implemented-memory-mechanisms">Methods</a>
  </p>
</div>

---

## üéØ Overview

AgentMemoryBench provides a unified framework to evaluate how LLM agents learn and retain two types of memory:
- **System Memory**: Task workflows and execution patterns
- **Personal Memory**: User preferences and dialogue context

The benchmark spans **6 interactive tasks** across 4 grounding types:
- **Code-grounded**: Database (SQL), Operating System (Shell), Knowledge Graph (SPARQL)
- **Embodied**: ALFWorld (household tasks)
- **Web-grounded**: WebShop (e-commerce)
- **Dialogue-grounded**: LoCoMo (long-term conversations)

## üìä Evaluation Modes

AgentMemoryBench supports **5 complementary evaluation modes** to provide multi-dimensional assessment of memory systems:

![Evaluation Modes](iclr2026/figures/evaluation_mode.png)

### 1. **Offline Mode**
Traditional train-test split evaluation. The agent learns from training samples (memory formation & evolution) and is tested on held-out samples (retrieval only).

**Metrics**: Average Success Rate (ASR), Average Steps (AS), F1-score, BLEU, LLM-as-Judge

### 2. **Online Mode**
Streaming evaluation where agents process samples sequentially with real-time memory updates. Performance is recorded after each sample to capture learning dynamics.

**Metrics**: Cumulative Success Rate (CSR), Learning Gain (LG), Stability Loss (SL)

### 3. **Replay Mode**
Periodic testing to measure knowledge retention and resistance to forgetting. After learning each stage, agents are tested on previously learned samples.

**Metrics**: Forgetting Rate (FR), Average Success Rate (ASR)

### 4. **Transfer Mode**
- **Cross-environment**: Tests knowledge generalization across different domains (e.g., DB‚ÜíOS)
- **Within-environment**: Measures forward transfer‚Äîhow learning current samples helps future ones

**Metrics**: Transfer Gain (TG), Forward Transfer Gain (FTG)

### 5. **Repair Mode**
Tests robustness and self-correction under erroneous feedback. Agents learn under incorrect rewards, then repair memory with correct feedback.

**Metrics**: Error Robustness (ER), Repair Gain (RG), Net Recovery (NR)

## üèóÔ∏è Project Structure

```
AgentMemoryBench/
‚îú‚îÄ‚îÄ configs/                    # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ assignment/            # Experiment configurations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ default.yaml       # Main experiment config
‚îÇ   ‚îú‚îÄ‚îÄ tasks/                 # Task-specific configs (6 tasks)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dbbench.yaml       # Database (SQL)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ os.yaml            # Operating System (Shell)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kg.yaml            # Knowledge Graph (SPARQL)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alfworld.yaml      # Embodied AI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ webshop.yaml       # E-commerce
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ locomo-*.yaml      # Long conversations (0-9)
‚îÇ   ‚îî‚îÄ‚îÄ llmapi/                # LLM API configurations
‚îÇ       ‚îú‚îÄ‚îÄ api.yaml           # API endpoint & key for agent LLM
‚îÇ       ‚îú‚îÄ‚îÄ agent.yaml         # Agent model name
‚îÇ       ‚îú‚îÄ‚îÄ evaluate_api.yaml  # API for LoCoMo LLM-as-Judge
‚îÇ       ‚îî‚îÄ‚îÄ evaluate_agent.yaml# Model for evaluation
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Task datasets
‚îÇ   ‚îú‚îÄ‚îÄ dbbench/               # Database operations (SQL)
‚îÇ   ‚îú‚îÄ‚îÄ os_interaction/        # OS commands (Shell)
‚îÇ   ‚îú‚îÄ‚îÄ knowledgegraph/        # KG queries (SPARQL)
‚îÇ   ‚îú‚îÄ‚îÄ alfworld/              # Embodied tasks
‚îÇ   ‚îú‚îÄ‚îÄ webshop/               # E-commerce tasks
‚îÇ   ‚îî‚îÄ‚îÄ locomo/                # Long dialogues (10 conversations)
‚îÇ
‚îú‚îÄ‚îÄ memory/                     # Memory mechanism implementations
‚îÇ   ‚îú‚îÄ‚îÄ base.py                # Base class for all memory mechanisms
‚îÇ   ‚îú‚îÄ‚îÄ registry.py            # Memory registry system
‚îÇ   ‚îú‚îÄ‚îÄ zero_shot/             # Baseline (no memory)
‚îÇ   ‚îú‚îÄ‚îÄ streamICL/             # RAG-based retrieval (topk=4)
‚îÇ   ‚îú‚îÄ‚îÄ awmPro/                # System memory via workflows (topk=8)
‚îÇ   ‚îú‚îÄ‚îÄ mem0/                  # Personal memory via preferences
‚îÇ   ‚îî‚îÄ‚îÄ MEMs/                  # Multi-memory coordination (proposed)
‚îÇ
‚îú‚îÄ‚îÄ execution/                  # Execution engines
‚îÇ   ‚îú‚îÄ‚îÄ base.py                # Base execution engine
‚îÇ   ‚îî‚îÄ‚îÄ single_agent/          # Single-agent executor
‚îÇ
‚îú‚îÄ‚îÄ src/                        # Core implementation
‚îÇ   ‚îú‚îÄ‚îÄ runner/                # Main entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Experiment runner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ builders.py        # Component builders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration parser
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schedule_utils.py  # Scheduling utilities
‚îÇ   ‚îú‚îÄ‚îÄ client/                # Client-side scheduling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backend.py         # Backend interface
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py       # Task scheduler
‚îÇ   ‚îú‚îÄ‚îÄ server/                # Backend task servers (Docker)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tasks/             # Task implementations
‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # Analysis utilities
‚îÇ       ‚îú‚îÄ‚îÄ message_schema.py  # Message format compatibility layer
‚îÇ       ‚îî‚îÄ‚îÄ analyze_results_*.py # Result analysis scripts
‚îÇ
‚îú‚îÄ‚îÄ extra/                      # Docker orchestration
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml     # Service definitions
‚îÇ   ‚îî‚îÄ‚îÄ *.Dockerfile           # Task-specific containers
‚îÇ
‚îú‚îÄ‚îÄ outputs/                    # Experiment results
‚îÇ   ‚îî‚îÄ‚îÄ [timestamp]/           # Grouped by experiment time
‚îÇ       ‚îî‚îÄ‚îÄ [task_name]/       # Grouped by task
‚îÇ           ‚îî‚îÄ‚îÄ [index].json   # Individual sample results
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îî‚îÄ‚îÄ README.md                   # This file
```

## üöÄ Quick Start

### 1. Prerequisites

#### Python Environment
```bash
# Create conda environment with Python 3.9
conda create -n aMB python=3.9

# Activate environment
conda activate aMB

# Navigate to project directory
cd /path/to/AgentMemoryBench

# Install dependencies
pip install -r requirements.txt
```

#### Docker Installation
Docker is required to run backend task servers. Install Docker Desktop:
- **Windows/Mac**: [Docker Desktop](https://www.docker.com/products/docker-desktop)
- **Linux**: Follow [official guide](https://docs.docker.com/engine/install/)

### 2. Data & Model Setup

#### Knowledge Graph (Freebase) Database

The Knowledge Graph task requires the Freebase database:

1. **Download database** (~50 GB):
   - Download link: [OneDrive](https://buckeyemailosu-my.sharepoint.com/:u:/g/personal/su_809_osu_edu/Ed0SY7sAS_ZGqNTovDYhVCcBxEmZfhL3B-chAiuoZCrpVg?e=vpHUei)
   - **Recommended**: Use a download manager (e.g., Free Download Manager) instead of browser

2. **Extract** the downloaded `virtuoso_db.zip`

3. **Configure path** in `extra/docker-compose.yml` (line 114):
   ```yaml
   freebase:
     build:
       context: ..
       dockerfile: extra/freebase.Dockerfile
     volumes:
       - "/absolute/path/to/virtuoso_db:/database"  # Use absolute path
     init: true
   ```

   **Important**:
   - Use **absolute paths**
   - Windows: Use forward slashes `/` (e.g., `C:/Users/...`)
   - Example: `B:/desktop/AgentMemoryBench/virtuoso_db:/database`

#### LoCoMo Tokenizer

Download the tokenizer model for fair evaluation:

```bash
# Download xlm-roberta-base from HuggingFace
# https://huggingface.co/FacebookAI/xlm-roberta-base

# Configure path in src/server/tasks/locomo/task.py (line 47)
tokenizer = AutoTokenizer.from_pretrained("/path/to/xlm-roberta-base")
```

#### Embedding Model (for streamICL, awmPro, MEMs)

Download the embedding model for fair comparison:

```bash
# Download bge-base-en-v1.5 from HuggingFace
# https://huggingface.co/BAAI/bge-base-en-v1.5

# Configure paths in YAML files:
# - memory/streamICL/streamICL.yaml
# - memory/awmPro/awmPro.yaml
# - memory/MEMs/MEMs.yaml
```

#### Mem0 API Key

To use the Mem0 method:

1. Register for API key at [mem0.ai](https://app.mem0.ai/)
2. Configure in `memory/mem0/mem0.yaml`:
   ```yaml
   api_key: "your_mem0_api_key_here"
   wait_time: 60.0  # Recommended: 60s for system tasks, 150s for personal, 100s for mixed
   ```

### 3. Start Backend Services

```bash
# Navigate to Docker directory
cd extra

# Build required containers
docker-compose build local-os-default
docker-compose build local-os-packages
docker-compose build local-os-ubuntu
docker-compose build freebase

# Start all services
docker-compose up
```

**Note**: Keep this terminal running. Services run on `http://localhost:5038`

### 4. Configure LLM API

**Recommended**: Use [SiliconFlow API](https://siliconflow.cn/) to avoid model name mismatches.

#### Agent LLM Configuration

Edit `configs/llmapi/api.yaml`:

```yaml
base_url: "https://api.siliconflow.cn/v1"
headers:
  Content-Type: application/json
  Authorization: "Bearer YOUR_API_KEY"
```

Edit `configs/llmapi/agent.yaml`:

```yaml
model: "Qwen/Qwen2.5-14B-Instruct"  # Or your preferred model
```

#### Evaluation LLM (for LoCoMo LLM-as-Judge)

Edit `configs/llmapi/evaluate_api.yaml`:

```yaml
base_url: "https://api.siliconflow.cn/v1"
headers:
  Content-Type: application/json
  Authorization: "Bearer YOUR_API_KEY"
```

Edit `configs/llmapi/evaluate_agent.yaml`:

```yaml
model: "Qwen/Qwen2.5-14B-Instruct"  # Or evaluation model
```

### 5. Configure Experiments

Edit `configs/assignment/default.yaml`:

```yaml
# Lifelong Learning Benchmark Configuration
# ÈÖçÁΩÆË¶ÅÊµãËØïÁöÑ‰ªªÂä°„ÄÅËÆ∞ÂøÜÊú∫Âà∂„ÄÅÊâßË°åÊñπÊ≥ïÂíåÂÆûÈ™åÂèÇÊï∞

# ===== ‰ªªÂä°ÈÖçÁΩÆ =====
# ÊåáÂÆöË¶ÅÊµãËØïÁöÑ‰ªªÂä°ÂàóË°®Ôºà5‰∏™system memory‰ªªÂä°+2‰∏™user memory‰ªªÂä°ÔºåÂÖ±7‰∏™‰ªªÂä°Ôºâ
tasks:
  # system memory‰ªªÂä°
  # - name: dbbench-std
  #   config_path: configs/tasks/dbbench.yaml
  - name: os-std
    config_path: configs/tasks/os.yaml
  # - name: kg-std
  #   config_path: configs/tasks/kg.yaml
  # - name: alfworld-std
  #   config_path: configs/tasks/alfworld.yaml
  # - name: webshop-std
  #   config_path: configs/tasks/webshop.yaml

  # user memory‰ªªÂä°
  # - name: locomo-0
  #   config_path: configs/tasks/locomo-0.yaml
  # - name: locomo-1
  #   config_path: configs/tasks/locomo-1.yaml
  # - name: locomo-2
  #   config_path: configs/tasks/locomo-2.yaml
  # - name: locomo-3
  #   config_path: configs/tasks/locomo-3.yaml
  # - name: locomo-4
  #   config_path: configs/tasks/locomo-4.yaml
  # - name: locomo-5
  #   config_path: configs/tasks/locomo-5.yaml
  # - name: locomo-6
  #   config_path: configs/tasks/locomo-6.yaml
  # - name: locomo-7
  #   config_path: configs/tasks/locomo-7.yaml
  # - name: locomo-8
  #   config_path: configs/tasks/locomo-8.yaml
  # - name: locomo-9
  #   config_path: configs/tasks/locomo-9.yaml

# ===== ËÆ∞ÂøÜÊú∫Âà∂ÈÖçÁΩÆ =====
# ‰ªé memory Êñá‰ª∂Â§π‰∏≠ÈÄâÊã©ËÆ∞ÂøÜÊú∫Âà∂ÔºàÁªü‰∏Ä‰ΩøÁî® snake_case ÂëΩÂêçÔºâ
memory_mechanism:
  name: zero_shot  # ÂèØÈÄâ: zero_shot, stream_icl, mem0, awm_pro

# ===== ÊâßË°åÊñπÊ≥ïÈÖçÁΩÆ =====
# ‰ªé execution Êñá‰ª∂Â§π‰∏≠ÈÄâÊã©ÊâßË°åÊñπÊ≥ï
execution_method:
  name: single_agent  # ÂΩìÂâçÁâàÊú¨‰ªÖÊîØÊåÅ single_agent
  config_path: execution/single_agent/single_agent.yaml

# ===== ÂÆûÈ™åÂèÇÊï∞ =====
experiment:
  # ËÆ≠ÁªÉÊ®°Âºè: online (Âú®Á∫øÂ≠¶‰π†) Êàñ offline (Á¶ªÁ∫øÂ≠¶‰π†) Êàñ replay (ÈáçÊîæÂ≠¶‰π†) Êàñ transfer (ËøÅÁßªÂ≠¶‰π†)  Êàñ repairÔºà‰øÆÂ§çÂ≠¶‰π†Ôºâ
  training_mode: online  # online | offline | replay | transfer | repair
  
  keep_number: 700 #Âè™Êúâtraining_modeÁ≠â‰∫éonlineÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïà #‰∏∫NoneÊàñËÄÖÂ∞è‰∫éÁ≠â‰∫é0ÔºåÂàô‰∏çËøõË°åÊà™Êñ≠

  train_size: 0.6 #Âè™Êúâtraining_modeÁ≠â‰∫éofflineÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïà

  #Âú®transfer_task‰∏≠Â≠¶‰π†Ôºàupdate+enhanceÔºåÁõ∏ÂΩì‰∫éonlineÔºâÔºåÂú®transfer_after_task‰∏≠ËøõË°åÊµãËØïÔºà‰ªÖenhanceÔºâ
  transfer_task: dbbench-std #Âè™Êúâtraining_modeÁ≠â‰∫étransferÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïà
  transfer_after_task: os-std #Âè™Êúâtraining_modeÁ≠â‰∫étransferÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïà
  forward_transfer_num: 3 #Âè™Êúâtraining_modeÁ≠â‰∫étransfer‰∏îtransfer_task==transfer_after_taskÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïàÔºåË°®Á§∫ÂâçÂêëËøÅÁßªÁöÑÊ≠•Êï∞

  #Ëøô‰∏§‰∏™ÂèÇÊï∞ÁöÑÊÑèÊÄùÊòØÔºåÊØèÂ≠¶Ëøám‰∏™Ê†∑Êú¨Ôºàupdate+enhanceÔºåÁõ∏ÂΩì‰∫éonlineÔºâÔºåÂ∞±‰ªéÂ≠¶ËøáÁöÑÊâÄÊúâÊ†∑Êú¨‰∏≠ÈöèÊú∫ÊäΩÊ†∑n‰∏™ËøõË°åÊµãËØïÔºà‰ªÖenhanceÔºâ
  replay_m: 20 #Âè™Êúâtraining_modeÁ≠â‰∫éreplayÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïà
  replay_n: 20 #Âè™Êúâtraining_modeÁ≠â‰∫éreplayÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïà
  replay_seed: 66 #Âè™Êúâtraining_modeÁ≠â‰∫éreplayÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïà

  #Ëøô‰∏§‰∏™ÂèÇÊï∞ÁöÑÊÑèÊÄùÊòØÔºåÂ∞ÜÊâÄÊúâÁöÑcaseÊåâÁÖßmÂàÜÊàêxÁªÑÔºåÁÑ∂ÂêéÁªÑ‰∏éÁªÑ‰πãÂâçÊòØ‰∏≤Ë°åÂ≠¶‰π†ÁöÑÔºåËøôÊ≤°ÊØõÁóÖÔºå‰ΩÜÊòØÊØè‰∏™ÁªÑ‰∏≠‰ºöÊúân‰∏™caseÁöÑjudgeÊòØÈîô‰π±ÁöÑ
  repair_m: 20  # Âè™Êúâtraining_modeÁ≠â‰∫érepairÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïàÔºàÂØπ‰∫éÊôÆÈÄö‰ªªÂä°ÔºâÔºåÊØèÁªÑÁöÑÊ†∑Êú¨Êï∞Èáè
  repair_n: 20  # Âè™Êúâtraining_modeÁ≠â‰∫érepairÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïàÔºåÊØèÁªÑ‰∏≠ÈúÄË¶ÅÂèçËΩ¨Â•ñÂä±ÁöÑÊ†∑Êú¨Êï∞Èáè
  repair_seed: 66  # Âè™Êúâtraining_modeÁ≠â‰∫érepairÊó∂ÔºåËøô‰∏™ÂèÇÊï∞ÊâçÊúâÊïàÔºåÈÄâÊã©ÂèçËΩ¨Ê†∑Êú¨ÁöÑÈöèÊú∫ÁßçÂ≠ê
  repair_size_locomo: 0.5  # Âè™Êúâtraining_modeÁ≠â‰∫érepair‰∏î‰ªªÂä°‰∏∫locomoÊó∂ÊúâÊïàÔºåË°®Á§∫ÊØè‰∏™session‰∏≠ÈúÄË¶ÅÂèçËΩ¨ÁöÑQAÊØî‰æãÔºà0-1‰πãÈó¥Ôºâ

  ...
  
  cross_task: False  # True | False

  # Êï∞ÊçÆÊâì‰π±: ÊòØÂê¶Êâì‰π±‰ªªÂä°È°∫Â∫èÔºåÂèØ‰ª•ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠ê
  shuffle:
    enabled: True  # True | False
    seed: 66  # Êï¥Êï∞ÔºåÂ¶ÇÊûú enabled ‰∏∫ true Êó∂‰ΩøÁî®
```

### 6. Run Experiments

```bash
# Run with default configuration
python -m src.runner.main

# Or specify custom config
python -m src.runner.main --config configs/assignment/my_experiment.yaml
```

## üõ†Ô∏è Creating Custom Memory Mechanisms

### Step 1: Implement Memory Class

Create a new directory under `memory/` (e.g., `memory/my_memory/`):

```python
# memory/my_memory/my_memory.py
from __future__ import annotations
from typing import List, Dict, Any
import yaml
from ..base import MemoryMechanism

class MyMemory(MemoryMechanism):
    """Your custom memory mechanism"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        # Initialize your memory storage

    def use_memory(
        self,
        task: str,
        messages: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Enhance messages with memory before LLM call.

        Args:
            task: Task name (e.g., "dbbench-std", "os-std")
            messages: Original message list

        Returns:
            Enhanced messages with retrieved memory
        """
        # Retrieve relevant experience from memory
        # Inject experience into messages
        return messages  # Return enhanced messages

    def update_memory(
        self,
        task: str,
        history: List[Dict[str, Any]],
        result: Dict[str, Any]
    ) -> None:
        """
        Update memory after sample execution.

        Args:
            task: Task name
            history: Full dialogue history
            result: Execution result (reward, status, etc.)
        """
        # Update your memory storage based on history and result
        pass

def load_my_memory_from_yaml(config_path: str) -> MyMemory:
    """Load memory from YAML config"""
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f) or {}
    return MyMemory(config)
```

Create configuration file `memory/my_memory/my_memory.yaml`:

```yaml
name: my_memory
description: "My custom memory mechanism"

# Your configuration parameters
param1: value1
param2: value2
```

### Step 2: Register in Registry

Add registration in `memory/registry.py`:

```python
def _register_all_memories():
    # ... existing registrations ...

    # Register your memory mechanism (use snake_case)
    from memory.my_memory.my_memory import load_my_memory_from_yaml
    register_memory(
        name="my_memory",  # Use snake_case
        loader_func=load_my_memory_from_yaml,
        default_config_path="memory/my_memory/my_memory.yaml",
    )
```

### Step 3: Use Your Memory

Configure in `configs/assignment/default.yaml`:

```yaml
memory_mechanism:
  name: my_memory  # Use snake_case naming
  config_path: memory/my_memory/my_memory.yaml  # Optional
```

## üìà Implemented Memory Mechanisms

| Method | Type | Description | Key Features |
|--------|------|-------------|--------------|
| **zero_shot** | Baseline | No memory | Reflects base LLM capability |
| **streamICL** | Retrieval | RAG-based ICL | Stores full trajectories, topk=4 |
| **awmPro** | System | Workflow memory | Extracts execution patterns, topk=8 |
| **mem0** | Personal | Preference memory | Graph-based storage with ADD/UPDATE/DELETE |
| **MEMs** | Hybrid | Multi-memory | Coordinates system & personal memory via trigger model |

### Fair Comparison Notes

- **streamICL**: Uses topk=4 following [original paper](https://arxiv.org/abs/2406.08747)
- **awmPro**: Modified from [AWM](https://arxiv.org/abs/2409.07429) with mem0-inspired management, topk=8 based on workflow induction experiments
- **mem0**: Uses best practices from [official implementation](https://arxiv.org/abs/2504.19413)

See ablation studies in paper for detailed topk analysis across different tasks.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Task datasets adapted from AgentBench and LoCoMo
- Evaluation protocols inspired by continual learning literature
- Memory baselines from StreamBench, AWM, and Mem0

---

**Project Status**: Active Development | **Latest Version**: v1.0.0 |